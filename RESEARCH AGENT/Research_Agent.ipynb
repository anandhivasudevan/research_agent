{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "id": "3807bf68",
      "cell_type": "code",
      "metadata": {
        "id": "3807bf68"
      },
      "execution_count": 1,
      "source": [
        "params = {\n",
        "    \"space_id\": \"2a36b3dc-8277-41c4-bb02-6c11fbe22ec9\",\n",
        "}\n",
        "\n",
        "\n",
        "def gen_ai_service(context, params = params, **custom):\n",
        "    # import dependencies\n",
        "    from langchain_ibm import ChatWatsonx\n",
        "    from ibm_watsonx_ai import APIClient\n",
        "    from ibm_watsonx_ai.foundation_models.utils import Tool, Toolkit\n",
        "    from langchain_core.messages import AIMessage, HumanMessage\n",
        "    from langgraph.checkpoint.memory import MemorySaver\n",
        "    from langgraph.prebuilt import create_react_agent\n",
        "    import json\n",
        "    import requests\n",
        "\n",
        "    model = \"meta-llama/llama-3-2-11b-vision-instruct\"\n",
        "\n",
        "    service_url = \"https://us-south.ml.cloud.ibm.com\"\n",
        "    # Get credentials token\n",
        "    credentials = {\n",
        "        \"url\": service_url,\n",
        "        \"token\": context.generate_token()\n",
        "    }\n",
        "\n",
        "    # Setup client\n",
        "    client = APIClient(credentials)\n",
        "    space_id = params.get(\"space_id\")\n",
        "    client.set.default_space(space_id)\n",
        "\n",
        "\n",
        "\n",
        "    def create_chat_model(watsonx_client):\n",
        "        parameters = {\n",
        "            \"frequency_penalty\": 0,\n",
        "            \"max_tokens\": 2000,\n",
        "            \"presence_penalty\": 0,\n",
        "            \"temperature\": 0,\n",
        "            \"top_p\": 1\n",
        "        }\n",
        "\n",
        "        chat_model = ChatWatsonx(\n",
        "            model_id=model,\n",
        "            url=service_url,\n",
        "            space_id=space_id,\n",
        "            params=parameters,\n",
        "            watsonx_client=watsonx_client,\n",
        "        )\n",
        "        return chat_model\n",
        "\n",
        "\n",
        "    def create_utility_agent_tool(tool_name, params, api_client, **kwargs):\n",
        "        from langchain_core.tools import StructuredTool\n",
        "        utility_agent_tool = Toolkit(\n",
        "            api_client=api_client\n",
        "        ).get_tool(tool_name)\n",
        "\n",
        "        tool_description = utility_agent_tool.get(\"description\")\n",
        "\n",
        "        if (kwargs.get(\"tool_description\")):\n",
        "            tool_description = kwargs.get(\"tool_description\")\n",
        "        elif (utility_agent_tool.get(\"agent_description\")):\n",
        "            tool_description = utility_agent_tool.get(\"agent_description\")\n",
        "\n",
        "        tool_schema = utility_agent_tool.get(\"input_schema\")\n",
        "        if (tool_schema == None):\n",
        "            tool_schema = {\n",
        "                \"type\": \"object\",\n",
        "                \"additionalProperties\": False,\n",
        "                \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
        "                \"properties\": {\n",
        "                    \"input\": {\n",
        "                        \"description\": \"input for the tool\",\n",
        "                        \"type\": \"string\"\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "\n",
        "        def run_tool(**tool_input):\n",
        "            query = tool_input\n",
        "            if (utility_agent_tool.get(\"input_schema\") == None):\n",
        "                query = tool_input.get(\"input\")\n",
        "\n",
        "            results = utility_agent_tool.run(\n",
        "                input=query,\n",
        "                config=params\n",
        "            )\n",
        "\n",
        "            return results.get(\"output\")\n",
        "\n",
        "        return StructuredTool(\n",
        "            name=tool_name,\n",
        "            description = tool_description,\n",
        "            func=run_tool,\n",
        "            args_schema=tool_schema\n",
        "        )\n",
        "\n",
        "\n",
        "    def create_custom_tool(tool_name, tool_description, tool_code, tool_schema, tool_params):\n",
        "        from langchain_core.tools import StructuredTool\n",
        "        import ast\n",
        "\n",
        "        def call_tool(**kwargs):\n",
        "            tree = ast.parse(tool_code, mode=\"exec\")\n",
        "            custom_tool_functions = [ x for x in tree.body if isinstance(x, ast.FunctionDef) ]\n",
        "            function_name = custom_tool_functions[0].name\n",
        "            compiled_code = compile(tree, 'custom_tool', 'exec')\n",
        "            namespace = tool_params if tool_params else {}\n",
        "            exec(compiled_code, namespace)\n",
        "            return namespace[function_name](**kwargs)\n",
        "\n",
        "        tool = StructuredTool(\n",
        "            name=tool_name,\n",
        "            description = tool_description,\n",
        "            func=call_tool,\n",
        "            args_schema=tool_schema\n",
        "        )\n",
        "        return tool\n",
        "\n",
        "    def create_custom_tools():\n",
        "        custom_tools = []\n",
        "\n",
        "\n",
        "    def create_tools(inner_client, context):\n",
        "        tools = []\n",
        "\n",
        "        config = None\n",
        "        tools.append(create_utility_agent_tool(\"GoogleSearch\", config, inner_client))\n",
        "        config = {\n",
        "        }\n",
        "        tools.append(create_utility_agent_tool(\"DuckDuckGo\", config, inner_client))\n",
        "        config = {\n",
        "            \"maxResults\": 5\n",
        "        }\n",
        "        tools.append(create_utility_agent_tool(\"Wikipedia\", config, inner_client))\n",
        "        config = {\n",
        "        }\n",
        "        tools.append(create_utility_agent_tool(\"WebCrawler\", config, inner_client))\n",
        "        config = {\n",
        "        }\n",
        "        tools.append(create_utility_agent_tool(\"Weather\", config, inner_client))\n",
        "        return tools\n",
        "\n",
        "    def create_agent(model, tools, messages):\n",
        "        memory = MemorySaver()\n",
        "        instructions = \"\"\"# Notes\n",
        "- Use markdown syntax for formatting code snippets, links, JSON, tables, images, files.\n",
        "- Any HTML tags must be wrapped in block quotes, for example ```<html>```.\n",
        "- When returning code blocks, specify language.\n",
        "- Sometimes, things don't go as planned. Tools may not provide useful information on the first few tries. You should always try a few different approaches before declaring the problem unsolvable.\n",
        "- When the tool doesn't give you what you were asking for, you must either use another tool or a different tool input.\n",
        "- When using search engines, you try different formulations of the query, possibly even in a different language.\n",
        "- You cannot do complex calculations, computations, or data manipulations without using tools.\n",
        "- If you need to call a tool to compute something, always call it instead of saying you will call it.\n",
        "\n",
        "If a tool returns an IMAGE in the result, you must include it in your answer as Markdown.\n",
        "\n",
        "Example:\n",
        "\n",
        "Tool result: IMAGE({commonApiUrl}/wx/v1-beta/utility_agent_tools/cache/images/plt-04e3c91ae04b47f8934a4e6b7d1fdc2c.png)\n",
        "Markdown to return to user: ![Generated image]({commonApiUrl}/wx/v1-beta/utility_agent_tools/cache/images/plt-04e3c91ae04b47f8934a4e6b7d1fdc2c.png)\n",
        "\n",
        "It can autonomously search for literature, summarize papers, and organize references.\n",
        " Using natural language processing, it understands research questions and retrieves relevant information.\n",
        "The agent can generate reports, suggest hypotheses, and even draft sections of research papers.\n",
        "It saves time by automating repetitive tasks like citation management and data extraction.\n",
        " Research Agents enhance efficiency, accuracy, and innovation in both academic and industrial R&D. Technology\"\"\"\n",
        "        for message in messages:\n",
        "            if message[\"role\"] == \"system\":\n",
        "                instructions += message[\"content\"]\n",
        "        graph = create_react_agent(model, tools=tools, checkpointer=memory, state_modifier=instructions)\n",
        "        return graph\n",
        "\n",
        "    def convert_messages(messages):\n",
        "        converted_messages = []\n",
        "        for message in messages:\n",
        "            if (message[\"role\"] == \"user\"):\n",
        "                converted_messages.append(HumanMessage(content=message[\"content\"]))\n",
        "            elif (message[\"role\"] == \"assistant\"):\n",
        "                converted_messages.append(AIMessage(content=message[\"content\"]))\n",
        "        return converted_messages\n",
        "\n",
        "    def generate(context):\n",
        "        payload = context.get_json()\n",
        "        messages = payload.get(\"messages\")\n",
        "        inner_credentials = {\n",
        "            \"url\": service_url,\n",
        "            \"token\": context.get_token()\n",
        "        }\n",
        "\n",
        "        inner_client = APIClient(inner_credentials)\n",
        "        model = create_chat_model(inner_client)\n",
        "        tools = create_tools(inner_client, context)\n",
        "        agent = create_agent(model, tools, messages)\n",
        "\n",
        "        generated_response = agent.invoke(\n",
        "            { \"messages\": convert_messages(messages) },\n",
        "            { \"configurable\": { \"thread_id\": \"42\" } }\n",
        "        )\n",
        "\n",
        "        last_message = generated_response[\"messages\"][-1]\n",
        "        generated_response = last_message.content\n",
        "\n",
        "        execute_response = {\n",
        "            \"headers\": {\n",
        "                \"Content-Type\": \"application/json\"\n",
        "            },\n",
        "            \"body\": {\n",
        "                \"choices\": [{\n",
        "                    \"index\": 0,\n",
        "                    \"message\": {\n",
        "                       \"role\": \"assistant\",\n",
        "                       \"content\": generated_response\n",
        "                    }\n",
        "                }]\n",
        "            }\n",
        "        }\n",
        "\n",
        "        return execute_response\n",
        "\n",
        "    def generate_stream(context):\n",
        "        print(\"Generate stream\", flush=True)\n",
        "        payload = context.get_json()\n",
        "        headers = context.get_headers()\n",
        "        is_assistant = headers.get(\"X-Ai-Interface\") == \"assistant\"\n",
        "        messages = payload.get(\"messages\")\n",
        "        inner_credentials = {\n",
        "            \"url\": service_url,\n",
        "            \"token\": context.get_token()\n",
        "        }\n",
        "        inner_client = APIClient(inner_credentials)\n",
        "        model = create_chat_model(inner_client)\n",
        "        tools = create_tools(inner_client, context)\n",
        "        agent = create_agent(model, tools, messages)\n",
        "\n",
        "        response_stream = agent.stream(\n",
        "            { \"messages\": messages },\n",
        "            { \"configurable\": { \"thread_id\": \"42\" } },\n",
        "            stream_mode=[\"updates\", \"messages\"]\n",
        "        )\n",
        "\n",
        "        for chunk in response_stream:\n",
        "            chunk_type = chunk[0]\n",
        "            finish_reason = \"\"\n",
        "            usage = None\n",
        "            if (chunk_type == \"messages\"):\n",
        "                message_object = chunk[1][0]\n",
        "                if (message_object.type == \"AIMessageChunk\" and message_object.content != \"\"):\n",
        "                    message = {\n",
        "                        \"role\": \"assistant\",\n",
        "                        \"content\": message_object.content\n",
        "                    }\n",
        "                else:\n",
        "                    continue\n",
        "            elif (chunk_type == \"updates\"):\n",
        "                update = chunk[1]\n",
        "                if (\"agent\" in update):\n",
        "                    agent = update[\"agent\"]\n",
        "                    agent_result = agent[\"messages\"][0]\n",
        "                    if (agent_result.additional_kwargs):\n",
        "                        kwargs = agent[\"messages\"][0].additional_kwargs\n",
        "                        tool_call = kwargs[\"tool_calls\"][0]\n",
        "                        if (is_assistant):\n",
        "                            message = {\n",
        "                                \"role\": \"assistant\",\n",
        "                                \"step_details\": {\n",
        "                                    \"type\": \"tool_calls\",\n",
        "                                    \"tool_calls\": [\n",
        "                                        {\n",
        "                                            \"id\": tool_call[\"id\"],\n",
        "                                            \"name\": tool_call[\"function\"][\"name\"],\n",
        "                                            \"args\": tool_call[\"function\"][\"arguments\"]\n",
        "                                        }\n",
        "                                    ]\n",
        "                                }\n",
        "                            }\n",
        "                        else:\n",
        "                            message = {\n",
        "                                \"role\": \"assistant\",\n",
        "                                \"tool_calls\": [\n",
        "                                    {\n",
        "                                        \"id\": tool_call[\"id\"],\n",
        "                                        \"type\": \"function\",\n",
        "                                        \"function\": {\n",
        "                                            \"name\": tool_call[\"function\"][\"name\"],\n",
        "                                            \"arguments\": tool_call[\"function\"][\"arguments\"]\n",
        "                                        }\n",
        "                                    }\n",
        "                                ]\n",
        "                            }\n",
        "                    elif (agent_result.response_metadata):\n",
        "                        # Final update\n",
        "                        message = {\n",
        "                            \"role\": \"assistant\",\n",
        "                            \"content\": agent_result.content\n",
        "                        }\n",
        "                        finish_reason = agent_result.response_metadata[\"finish_reason\"]\n",
        "                        if (finish_reason):\n",
        "                            message[\"content\"] = \"\"\n",
        "\n",
        "                        usage = {\n",
        "                            \"completion_tokens\": agent_result.usage_metadata[\"output_tokens\"],\n",
        "                            \"prompt_tokens\": agent_result.usage_metadata[\"input_tokens\"],\n",
        "                            \"total_tokens\": agent_result.usage_metadata[\"total_tokens\"]\n",
        "                        }\n",
        "                elif (\"tools\" in update):\n",
        "                    tools = update[\"tools\"]\n",
        "                    tool_result = tools[\"messages\"][0]\n",
        "                    if (is_assistant):\n",
        "                        message = {\n",
        "                            \"role\": \"assistant\",\n",
        "                            \"step_details\": {\n",
        "                                \"type\": \"tool_response\",\n",
        "                                \"id\": tool_result.id,\n",
        "                                \"tool_call_id\": tool_result.tool_call_id,\n",
        "                                \"name\": tool_result.name,\n",
        "                                \"content\": tool_result.content\n",
        "                            }\n",
        "                        }\n",
        "                    else:\n",
        "                        message = {\n",
        "                            \"role\": \"tool\",\n",
        "                            \"id\": tool_result.id,\n",
        "                            \"tool_call_id\": tool_result.tool_call_id,\n",
        "                            \"name\": tool_result.name,\n",
        "                            \"content\": tool_result.content\n",
        "                        }\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "            chunk_response = {\n",
        "                \"choices\": [{\n",
        "                    \"index\": 0,\n",
        "                    \"delta\": message\n",
        "                }]\n",
        "            }\n",
        "            if (finish_reason):\n",
        "                chunk_response[\"choices\"][0][\"finish_reason\"] = finish_reason\n",
        "            if (usage):\n",
        "                chunk_response[\"usage\"] = usage\n",
        "            yield chunk_response\n",
        "\n",
        "    return generate, generate_stream\n"
      ],
      "outputs": []
    }
  ]
}